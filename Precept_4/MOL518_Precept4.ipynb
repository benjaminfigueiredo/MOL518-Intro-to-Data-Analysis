{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In colab run this cell first to setup the file structure!\n",
    "%cd /content\n",
    "!rm -rf MOL518-Intro-to-Data-Analysis\n",
    "\n",
    "!git clone https://github.com/benjaminfigueiredo/MOL518-Intro-to-Data-Analysis.git\n",
    "%cd MOL518-Intro-to-Data-Analysis/Precept_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b20ef9",
   "metadata": {},
   "source": [
    "# Hypothesis testing for empirical distributions\n",
    "\n",
    "In this precept, we will use a real dataset to explore concepts you've learned in class relating to random waiting times and statistical hypothesis testing.\n",
    "\n",
    "## The exponential random distribution and waiting times\n",
    "\n",
    "When a random event occurs at a constant rate $\\lambda$ per unit time, the waiting time for an event occurence is an exponential random variable: the probability of a wait time $T$ exceeding a given threshold decays exponentially in the threshold, i.e.\n",
    "\n",
    "$$ P[T > t] = \\exp(-\\lambda t). $$\n",
    "\n",
    "From this, it can be shown that the mean and standard deviation of the waiting time are $E[T] = \\sigma_T = \\lambda^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed096c63",
   "metadata": {},
   "source": [
    "## Counting earthquakes in California\n",
    "\n",
    "### Background\n",
    "\n",
    "![san_andreas_fault](figs/san_andreas_fault.png)\n",
    "\n",
    "Suppose you decide to spend your vacation in California, so you sit in a cafe and note down when earthquakes happen. If you do this exercise, you will notice that earthquakes are, in multiple ways, random events: they occur at unpredictable times and have unpredictable strengths (say in terms of how much shaking you observe). \n",
    "\n",
    "Modern geophysics gives us a clear idea as to why earthquakes happen: the solid ground we sit on (the Earth's outer layer, called the lithosphere) is actually made up of disjoint tectonic plates that fit together like puzzle pieces sitting atop a ductile (in some ways fluid-like) mantle. Since these pieces are being advected by the mantle in different directions, stress tends to build up at the boundaries between plates. When this stress is released, part of the energy is dissipated into acoustic waves, and the ground itself shakes. \n",
    "\n",
    "While this is verbally simple enough, earthquakes are unpredictable, owing among other reasons to the complex geometry of plate contact and the non-linear dynamics of stress transfer between plates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d1206",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129954b",
   "metadata": {},
   "source": [
    "In data/earthquake.csv, you can find a dataset of timestamps of magnitude 3+ earthquakes in the vicinity of the state of California (see supplement for more information on this particular data). This dataset is a csv table with two columns: time, giving a series of timestamps in seconds, and magnitude, which gives the magnitude of the earthquake on the Richter scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d392b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we read in the earthquake.csv file\n",
    "all_data = np.genfromtxt('data/earthquake.csv', delimiter=',', skip_header=True)\n",
    "timestamps = all_data[:, 0]\n",
    "magnitudes = all_data[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0325b",
   "metadata": {},
   "source": [
    "### Example 1: Gutenberg-Richter law\n",
    "\n",
    "According to the Gutenberg-Richter (GR) law, the number of earthquakes above a given magnitude $m = m_0 + \\Delta m$, where $m_0$ is a minimum detection threshold, decays exponentially relative to the threshold. As an aside, this has nothing to do with the constant rate assumptions underlying exponential distributions of waiting times, and reflects a power-law behavior in the distribution of radiated energy of an earthquake. Nevertheless, in terms of an occurence with random magnitude $M$, we can write, as per the intro, an exponential law:\n",
    "\n",
    "$$ P[M - m_0 > \\Delta m] = \\exp(- b \\;\\Delta m),$$\n",
    "\n",
    "where $b$ here is a rate parameter associated to earthquake magnitude. Let's see whether this law can be verified in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As noted in the intro, in an exponentially distributed random variable, the mean is equal to the standard deviation. \n",
    "# We can check this for our earthquake magnitudes.\n",
    "# First, we shift the magnitudes relative to the minimum we defined above, which is 3.\n",
    "magnitude_shifted = magnitudes - magnitudes.min()\n",
    "# From this dataset, we can estimate the mean and standard deviation of the magnitudes.\n",
    "mean_magnitude = np.mean(magnitude_shifted)\n",
    "std_magnitude = np.std(magnitudes)\n",
    "# Now we print out these estimates with two digit precision\n",
    "print(f'Mean shifted magnitude: {mean_magnitude:.2f}')\n",
    "print(f'Standard deviation of magnitude: {std_magnitude:.2f}')\n",
    "print(f'CV of earthquake magnitude: {std_magnitude / mean_magnitude:.2f}')\n",
    "\n",
    "# Now we plot the histogram of the shifted magnitudes\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(magnitude_shifted, bins=30)\n",
    "plt.title('Histogram of Shifted Earthquake Magnitudes')\n",
    "plt.xlabel('Shifted Magnitude')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e390a9b",
   "metadata": {},
   "source": [
    "Does this look like an exponential distribution? How accurate is our estimate of the mean and standard deviation from the above dataset? Would the result above satisfactorily answer whether the GR law is observed in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc973c0c",
   "metadata": {},
   "source": [
    "### Interlude: the Kolmogorov-Smirnov test statistic\n",
    "\n",
    "As we discussed in class, a more rigorous way to go about this is to test the hypothesis that our data obeys the GR law by defining a $p$-value, for which we need two things: first a test statistic $D$, which is a real-valued function over the space of our data. Second, we need an estimate of the distribution of $D$ under a null-hypothesis $H_0$, our baseline probability distribution for the data. Crucially, since $D$ is real-valued, we can exploit the ordering of the real numbers to define a single probability that encapsulates how likely of an outlier our observations are under the null-hypothesis, $p = P_{H_0}[D > \\hat{D}(\\mathrm{Data})]$.\n",
    "\n",
    "For an arbitrary empirical one-dimensional probability distribution, a test statistic we can define is called the **Kolmogorov-Smirnov statistic**. It can be defined in terms of the **complementary cumulative distribution function** (CCDF), $\\tilde{F}$, which for a random variable $X$ is a function we have been already a lot implicitly, namely $$\\tilde{F}(x) = P[X > x].$$ \n",
    "An estimator for the CDF is constructed by simply counting the proportion of observations above a value, i.e.,\n",
    "$$\\hat{\\tilde{F}}(x) = \\frac{\\#(\\mathrm{Data} > x)}{\\#(\\mathrm{Data})}.$$\n",
    "Then, we can define the Kolmogorov-Smirnov (KS) statistic $D$ as the largest deviation of $\\hat{\\tilde F}$ from $\\tilde{F}$ under the null hypothesis,\n",
    "$$D = \\max_{x \\in \\mathrm{Data}} | \\tilde{F}(x | H_0) - \\hat{\\tilde{F}}(x)|.$$\n",
    "\n",
    "**Take the time to convince yourself that this has all the properties we would expect from a test statistic.** Now we are going to define below these functions in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a0675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the CCDF of an exponential distribution as per above.\n",
    "def exponential_ccdf(x, rate):\n",
    "    return np.exp(-rate * x)\n",
    "\n",
    "def empirical_ccdf(data):\n",
    "    # Why does this simple snippet compute the empirical CCDF?\n",
    "    # Remember this is the image of the CCDF, not its domain values, which are the sorted data.\n",
    "    # Convince yourself this is correct! \n",
    "    n = len(data)\n",
    "    return 1 - np.arange(1, n + 1) / n \n",
    "\n",
    "def exponential_ks_statistic(data, rate):\n",
    "    sorted_data = np.sort(data)\n",
    "    # Explicitly compute the KS statistic\n",
    "    ks_statistic = np.max(np.abs(empirical_ccdf(data) - exponential_ccdf(sorted_data, rate)))\n",
    "    \n",
    "    return ks_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b388aa",
   "metadata": {},
   "source": [
    "PS: most sources are going to define this in terms of the cumulative distribution function (CDF), $F = 1 - \\tilde{F}$, but the math ends up being the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee7e5c",
   "metadata": {},
   "source": [
    "### Example 2: Parametric bootstrapping and a p-value for the GR hypothesis\n",
    "\n",
    "If we are happy with the Kolmogorov-Smirnov statistic $D$, all that remains is to compute what its expected distribution would be under the GR law, which we take to be our null hypothesis, to get a p-value. Indeed, this itself presents some challenges: (i) the GR law still doesn't satisfy our definition of a null-hypothesis, since all it specifies is a parametric family of distributions as a function of the rate $b$. (ii) Even if we know $b$, we don't know of a closed form for the distribution of the KS statistic of $5497$ independent identical realizations of the same exponential law. Are we doomed? \n",
    "\n",
    "Here we can employ a bootstrapping procedure. As you saw in class, bootstrapping involves getting more data out of your data. In our case, we can estimate the relevant rate parameter, which under the null hypothesis is nothing but $\\hat{b} = 1 / \\bar{M}$. If we are satisfied with this estimate, we can now simulate a bunch of exponentially distributed data with rate $\\hat{b}$ and compute the empirical distribution of the $KS$ statistic under this simulated data, $D^*$, to compare with our observed $KS$ statistic, $\\hat D$. This is called **parametric bootstrapping**, it is a self-consistent way to use an estimated distribution to validate the estimate of that distribution. Just like the $CCDF$, we can now compute the $p$-value under $R$ realizations of bootstrapping as\n",
    "\n",
    "$$p = \\frac{\\#(D^* > D)}{R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we compute the estimated rate parameter for the exponential distribution, which is the inverse of the mean.\n",
    "empirical_rate = 1 / mean_magnitude \n",
    "# We compute the KS statistic for our data and the exponential distribution with this rate parameter.\n",
    "ks_statistic = exponential_ks_statistic(magnitude_shifted, empirical_rate)\n",
    "print(f'KS statistic for earthquake magnitudes: {ks_statistic:.4f}')\n",
    "\n",
    "# Now the parametric bootstrap procedure to compute a p-value for the KS statistic.\n",
    "R = 1000 # number of bootstrap realizations\n",
    "bootstrap_ks_statistics = np.zeros(R)\n",
    "for i in range(R):\n",
    "    # We generate a bootstrap sample from the exponential distribution with the estimated rate parameter.\n",
    "    # Just note that np.random_exponential takes the scale parameter, which is the inverse of the rate parameter.\n",
    "    bootstrap_sample = np.random.exponential(scale=1/empirical_rate, size=len(magnitude_shifted))\n",
    "    # We compute the KS statistic for this bootstrap sample.\n",
    "    bootstrap_ks_statistics[i] = exponential_ks_statistic(bootstrap_sample, empirical_rate)\n",
    "# Finally, we compute the p-value as defined above,\n",
    "p = (1 + np.sum(bootstrap_ks_statistics >= ks_statistic)) / (1+R)\n",
    "print(f'Bootstrap p-value for KS statistic: {p:2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93665b36",
   "metadata": {},
   "source": [
    "This should make the GR hypothesis look pretty dire! Why is our p-value based hypothesis testing rejecting it so strongly? Let's do an obvious sanity check we should have done from the start: looking at the CCDF in log-scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d594a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we plot the empirical CCDF of the shifted magnitudes and the CCDF of the exponential distribution with the estimated rate parameter.\n",
    "x = np.linspace(0, magnitude_shifted.max(), 100)\n",
    "plt.plot(x, exponential_ccdf(x, empirical_rate), label='Exponential fit CCDF', color='orange', linestyle='--')\n",
    "plt.scatter(np.sort(magnitude_shifted), empirical_ccdf(magnitude_shifted), label='Empirical CCDF')\n",
    "plt.xlabel('Shifted magnitude')\n",
    "plt.ylabel('CCDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7eb1ab",
   "metadata": {},
   "source": [
    "### Exercise: saving the GR law\n",
    "\n",
    "From the plot above, why do you think the exponential hypothesis is being rejected? Is there a subset of the data for which it should hold? If so, verify it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ba9d4",
   "metadata": {},
   "source": [
    "# Bonus exercise: The distribution of times between earthquakes\n",
    "\n",
    "A harder problem (which starts to touch the complicated geophysics of earthquakes) is to consider the inter-arrival time of earthquakes. Now, if earthquakes were to happen at a constant rate $\\lambda$, we would expect these arrival times to behave a lot like the an exponential.\n",
    "\n",
    "To compute the intervals between events, we can use the diff function in numpy, which creates an array of successive differences of different arrays. For example, I compute below the total observed intervals between earthquakes in our dataset, in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = np.diff(timestamps)/(60*60*24) # convert to days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42eff57",
   "metadata": {},
   "source": [
    "Play around with the above data. Does it look exponential? How non-exponential is it, is it worse than the GR case above? How does it fare under hypothesis testing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
